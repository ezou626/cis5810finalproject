import numpy as np
import os, time, sys
import asyncio
import httpx
import base64
import cv2
from dotenv import load_dotenv
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def generate_caption_with_blip(image: np.ndarray, width: int, height: int) -> str:
    """
    Generate a caption for a single image using BLIP.

    Args:
        image (np.ndarray): The image as a NumPy array.

    Returns:
        str: The generated caption.
    """

    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    image = image.resize((width, height), Image.Resampling.LANCZOS)

    # Preprocess the image and generate the caption
    inputs = processor(images=image, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=20)

    # Decode and return the caption
    return processor.decode(outputs[0], skip_special_tokens=True)

def caption_images_with_blip(images, width: int, height: int):
    """
    Generate captions for a batch of images using BLIP.

    Args:
        images (list[np.ndarray]): A list of images as NumPy arrays.

    Returns:
        list[str]: A list of generated captions.
    """
    for image in images:
        caption = generate_caption_with_blip(image, width, height)
        yield caption